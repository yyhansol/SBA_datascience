{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5]\n",
    "y_data = [10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_3:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_1:0\", dtype=float32)\n",
      "Tensor(\"Y_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = W*X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(score-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'GradientDescent_2' type=NoOp>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)\n",
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss = 1113.19434 W =   2.107532 B =   0.771076\n",
      "1 Loss = 649.28186  W =   3.797611 B =   1.229203\n",
      "2 Loss = 378.93292  W =   5.088384 B =   1.576762\n",
      "3 Loss = 221.38313  W =   6.074334 B =   1.839924\n",
      "4 Loss = 129.56717  W =   6.827585 B =   2.038665\n",
      "5 Loss = 76.05760   W =   7.403196 B =   2.188237\n",
      "6 Loss = 44.87113   W =   7.843199 B =   2.300280\n",
      "7 Loss = 26.69350   W =   8.179678 B =   2.383683\n",
      "8 Loss = 16.09682   W =   8.437128 B =   2.445228\n",
      "9 Loss = 9.91793    W =   8.634246 B =   2.490096\n",
      "10 Loss = 6.31355    W =   8.785306 B =   2.522239\n",
      "11 Loss = 4.20949    W =   8.901204 B =   2.544676\n",
      "12 Loss = 2.97977    W =   8.990258 B =   2.559711\n",
      "13 Loss = 2.25960    W =   9.058819 B =   2.569101\n",
      "14 Loss = 1.83639    W =   9.111732 B =   2.574190\n",
      "15 Loss = 1.58625    W =   9.152700 B =   2.576002\n",
      "16 Loss = 1.43700    W =   9.184546 B =   2.575320\n",
      "17 Loss = 1.34656    W =   9.209427 B =   2.572741\n",
      "18 Loss = 1.29042    W =   9.228989 B =   2.568720\n",
      "19 Loss = 1.25429    W =   9.244488 B =   2.563606\n",
      "20 Loss = 1.22985    W =   9.256884 B =   2.557665\n",
      "21 Loss = 1.21224    W =   9.266910 B =   2.551099\n",
      "22 Loss = 1.19863    W =   9.275124 B =   2.544062\n",
      "23 Loss = 1.18737    W =   9.281953 B =   2.536674\n",
      "24 Loss = 1.17751    W =   9.287723 B =   2.529023\n",
      "25 Loss = 1.16849    W =   9.292683 B =   2.521179\n",
      "26 Loss = 1.15998    W =   9.297022 B =   2.513195\n",
      "27 Loss = 1.15178    W =   9.300885 B =   2.505109\n",
      "28 Loss = 1.14380    W =   9.304384 B =   2.496954\n",
      "29 Loss = 1.13595    W =   9.307603 B =   2.488752\n",
      "30 Loss = 1.12821    W =   9.310605 B =   2.480521\n",
      "31 Loss = 1.12055    W =   9.313440 B =   2.472274\n",
      "32 Loss = 1.11296    W =   9.316147 B =   2.464022\n",
      "33 Loss = 1.10543    W =   9.318753 B =   2.455773\n",
      "34 Loss = 1.09797    W =   9.321281 B =   2.447532\n",
      "35 Loss = 1.09055    W =   9.323748 B =   2.439305\n",
      "36 Loss = 1.08318    W =   9.326165 B =   2.431094\n",
      "37 Loss = 1.07587    W =   9.328544 B =   2.422902\n",
      "38 Loss = 1.06861    W =   9.330890 B =   2.414731\n",
      "39 Loss = 1.06139    W =   9.333210 B =   2.406583\n",
      "40 Loss = 1.05423    W =   9.335509 B =   2.398459\n",
      "41 Loss = 1.04711    W =   9.337790 B =   2.390359\n",
      "42 Loss = 1.04004    W =   9.340055 B =   2.382285\n",
      "43 Loss = 1.03302    W =   9.342305 B =   2.374236\n",
      "44 Loss = 1.02605    W =   9.344543 B =   2.366213\n",
      "45 Loss = 1.01912    W =   9.346771 B =   2.358216\n",
      "46 Loss = 1.01224    W =   9.348989 B =   2.350245\n",
      "47 Loss = 1.00541    W =   9.351196 B =   2.342301\n",
      "48 Loss = 0.99862    W =   9.353395 B =   2.334383\n",
      "49 Loss = 0.99188    W =   9.355585 B =   2.326492\n",
      "50 Loss = 0.98519    W =   9.357767 B =   2.318627\n",
      "51 Loss = 0.97853    W =   9.359941 B =   2.310788\n",
      "52 Loss = 0.97193    W =   9.362106 B =   2.302976\n",
      "53 Loss = 0.96537    W =   9.364264 B =   2.295190\n",
      "54 Loss = 0.95885    W =   9.366415 B =   2.287431\n",
      "55 Loss = 0.95238    W =   9.368558 B =   2.279697\n",
      "56 Loss = 0.94595    W =   9.370693 B =   2.271990\n",
      "57 Loss = 0.93956    W =   9.372821 B =   2.264308\n",
      "58 Loss = 0.93322    W =   9.374942 B =   2.256653\n",
      "59 Loss = 0.92692    W =   9.377055 B =   2.249023\n",
      "60 Loss = 0.92066    W =   9.379162 B =   2.241420\n",
      "61 Loss = 0.91445    W =   9.381261 B =   2.233841\n",
      "62 Loss = 0.90828    W =   9.383353 B =   2.226289\n",
      "63 Loss = 0.90215    W =   9.385438 B =   2.218762\n",
      "64 Loss = 0.89606    W =   9.387516 B =   2.211261\n",
      "65 Loss = 0.89001    W =   9.389586 B =   2.203784\n",
      "66 Loss = 0.88400    W =   9.391650 B =   2.196334\n",
      "67 Loss = 0.87803    W =   9.393707 B =   2.188908\n",
      "68 Loss = 0.87210    W =   9.395758 B =   2.181507\n",
      "69 Loss = 0.86622    W =   9.397800 B =   2.174132\n",
      "70 Loss = 0.86037    W =   9.399837 B =   2.166781\n",
      "71 Loss = 0.85456    W =   9.401866 B =   2.159455\n",
      "72 Loss = 0.84879    W =   9.403888 B =   2.152154\n",
      "73 Loss = 0.84306    W =   9.405903 B =   2.144878\n",
      "74 Loss = 0.83737    W =   9.407911 B =   2.137626\n",
      "75 Loss = 0.83172    W =   9.409913 B =   2.130399\n",
      "76 Loss = 0.82611    W =   9.411908 B =   2.123196\n",
      "77 Loss = 0.82053    W =   9.413897 B =   2.116018\n",
      "78 Loss = 0.81499    W =   9.415878 B =   2.108863\n",
      "79 Loss = 0.80949    W =   9.417853 B =   2.101733\n",
      "80 Loss = 0.80402    W =   9.419822 B =   2.094628\n",
      "81 Loss = 0.79859    W =   9.421783 B =   2.087546\n",
      "82 Loss = 0.79320    W =   9.423738 B =   2.080488\n",
      "83 Loss = 0.78785    W =   9.425687 B =   2.073454\n",
      "84 Loss = 0.78253    W =   9.427629 B =   2.066444\n",
      "85 Loss = 0.77725    W =   9.429564 B =   2.059457\n",
      "86 Loss = 0.77200    W =   9.431492 B =   2.052494\n",
      "87 Loss = 0.76679    W =   9.433414 B =   2.045555\n",
      "88 Loss = 0.76161    W =   9.435330 B =   2.038639\n",
      "89 Loss = 0.75647    W =   9.437240 B =   2.031746\n",
      "90 Loss = 0.75137    W =   9.439142 B =   2.024877\n",
      "91 Loss = 0.74630    W =   9.441038 B =   2.018031\n",
      "92 Loss = 0.74125    W =   9.442927 B =   2.011208\n",
      "93 Loss = 0.73625    W =   9.444811 B =   2.004408\n",
      "94 Loss = 0.73128    W =   9.446688 B =   1.997631\n",
      "95 Loss = 0.72635    W =   9.448559 B =   1.990877\n",
      "96 Loss = 0.72144    W =   9.450423 B =   1.984146\n",
      "97 Loss = 0.71657    W =   9.452281 B =   1.977438\n",
      "98 Loss = 0.71173    W =   9.454133 B =   1.970752\n",
      "99 Loss = 0.70693    W =   9.455978 B =   1.964089\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100):\n",
    "        _, loss_val = sess.run([train_op, loss], feed_dict={X:x_data, Y:y_data})\n",
    "        print(step, 'Loss = %-10.5f' % loss_val, \n",
    "              'W = %10.6f' % sess.run(W), 'B = %10.6f' % sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1129.9963 [2.1798773] [0.2959423]\n",
      "10 5.994956 [8.904748] [2.0766397]\n",
      "20 0.8597721 [9.375675] [2.1278253]\n",
      "30 0.7820427 [9.425663] [2.0650566]\n",
      "40 0.73072934 [9.446752] [1.9968337]\n",
      "50 0.6828729 [9.465306] [1.9303759]\n",
      "60 0.63815093 [9.48312] [1.8660975]\n",
      "70 0.5963572 [9.500333] [1.8039567]\n",
      "80 0.55730116 [9.516973] [1.743885]\n",
      "90 0.5208036 [9.533057] [1.6858135]\n",
      "100 0.4866957 [9.548606] [1.6296761]\n",
      "110 0.45482144 [9.563637] [1.5754081]\n",
      "120 0.42503494 [9.578168] [1.5229473]\n",
      "130 0.39719877 [9.592216] [1.4722337]\n",
      "140 0.37118584 [9.605794] [1.4232082]\n",
      "150 0.34687686 [9.618921] [1.3758155]\n",
      "160 0.3241596 [9.631611] [1.3300012]\n",
      "170 0.3029302 [9.643878] [1.2857126]\n",
      "180 0.28309107 [9.655737] [1.2428987]\n",
      "190 0.26455134 [9.667201] [1.2015104]\n",
      "200 0.24722528 [9.678283] [1.1614997]\n",
      "210 0.23103356 [9.688996] [1.1228219]\n",
      "220 0.21590345 [9.699352] [1.085432]\n",
      "230 0.2017636 [9.709364] [1.0492876]\n",
      "240 0.18855031 [9.719042] [1.0143467]\n",
      "250 0.17620249 [9.728397] [0.9805695]\n",
      "260 0.16466267 [9.737442] [0.947917]\n",
      "270 0.153879 [9.746184] [0.9163518]\n",
      "280 0.14380077 [9.754638] [0.8858374]\n",
      "290 0.13438357 [9.762808] [0.856339]\n",
      "300 0.12558271 [9.770706] [0.82782304]\n",
      "310 0.11735848 [9.778342] [0.80025667]\n",
      "320 0.10967199 [9.785723] [0.77360815]\n",
      "330 0.10248935 [9.792858] [0.7478472]\n",
      "340 0.09577798 [9.799756] [0.7229441]\n",
      "350 0.0895052 [9.806424] [0.6988704]\n",
      "360 0.0836436 [9.81287] [0.67559814]\n",
      "370 0.078165635 [9.819101] [0.65310097]\n",
      "380 0.07304661 [9.825125] [0.6313531]\n",
      "390 0.06826261 [9.830948] [0.6103292]\n",
      "400 0.06379237 [9.836577] [0.5900057]\n",
      "410 0.0596146 [9.842021] [0.5703585]\n",
      "420 0.05571019 [9.8472805] [0.5513654]\n",
      "430 0.052061357 [9.852366] [0.53300494]\n",
      "440 0.048652373 [9.857283] [0.51525575]\n",
      "450 0.045466047 [9.862035] [0.49809772]\n",
      "460 0.042487957 [9.866629] [0.48151132]\n",
      "470 0.03970561 [9.87107] [0.46547735]\n",
      "480 0.03710542 [9.875363] [0.4499771]\n",
      "490 0.034675177 [9.879514] [0.43499297]\n",
      "500 0.032403998 [9.883526] [0.4205078]\n",
      "510 0.03028216 [9.887404] [0.4065049]\n",
      "520 0.02829892 [9.891154] [0.39296836]\n",
      "530 0.026445517 [9.894778] [0.37988234]\n",
      "540 0.024713695 [9.898282] [0.36723244]\n",
      "550 0.02309507 [9.9016695] [0.35500386]\n",
      "560 0.021582838 [9.904943] [0.3431824]\n",
      "570 0.020168995 [9.908109] [0.33175462]\n",
      "580 0.01884845 [9.911168] [0.32070756]\n",
      "590 0.017613899 [9.914127] [0.3100282]\n",
      "600 0.016460668 [9.916986] [0.29970452]\n",
      "610 0.015382394 [9.919751] [0.28972468]\n",
      "620 0.014375061 [9.922424] [0.28007662]\n",
      "630 0.013433616 [9.925007] [0.27074984]\n",
      "640 0.012553724 [9.927505] [0.2617338]\n",
      "650 0.0117315985 [9.929918] [0.2530178]\n",
      "660 0.010963299 [9.932252] [0.24459238]\n",
      "670 0.0102451 [9.934508] [0.23644742]\n",
      "680 0.0095744245 [9.936689] [0.22857356]\n",
      "690 0.008947261 [9.938797] [0.22096196]\n",
      "700 0.008361252 [9.940835] [0.21360382]\n",
      "710 0.007813694 [9.942805] [0.20649076]\n",
      "720 0.007302 [9.94471] [0.19961476]\n",
      "730 0.0068237907 [9.94655] [0.19296779]\n",
      "740 0.006376926 [9.948331] [0.18654202]\n",
      "750 0.0059592063 [9.950051] [0.18033016]\n",
      "760 0.005569072 [9.9517145] [0.1743252]\n",
      "770 0.005204375 [9.953322] [0.1685201]\n",
      "780 0.004863499 [9.954877] [0.1629084]\n",
      "790 0.004545025 [9.956379] [0.15748389]\n",
      "800 0.0042473385 [9.957832] [0.15223975]\n",
      "810 0.0039690463 [9.959236] [0.14717011]\n",
      "820 0.0037090995 [9.960594] [0.1422693]\n",
      "830 0.003466282 [9.961906] [0.13753152]\n",
      "840 0.0032391518 [9.963175] [0.13295163]\n",
      "850 0.0030270875 [9.964401] [0.12852411]\n",
      "860 0.0028288835 [9.965587] [0.12424409]\n",
      "870 0.0026435156 [9.966732] [0.12010659]\n",
      "880 0.0024704193 [9.96784] [0.11610699]\n",
      "890 0.0023086667 [9.968911] [0.11224069]\n",
      "900 0.0021574344 [9.969946] [0.1085033]\n",
      "910 0.002016149 [9.970947] [0.10489023]\n",
      "920 0.001884078 [9.971915] [0.10139721]\n",
      "930 0.0017607163 [9.97285] [0.09802071]\n",
      "940 0.0016454421 [9.973754] [0.09475657]\n",
      "950 0.0015376755 [9.9746275] [0.09160131]\n",
      "960 0.0014369929 [9.975472] [0.0885512]\n",
      "970 0.0013428504 [9.97629] [0.08560245]\n",
      "980 0.0012548736 [9.977078] [0.08275193]\n",
      "990 0.0011726769 [9.977842] [0.07999641]\n",
      "1000 0.0010959045 [9.9785795] [0.07733273]\n",
      "Y = [9.9785795] * X + [0.07733273]\n",
      "X:5, Y: [49.97023]\n",
      "X:2.5, Y: [25.023783]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1001):\n",
    "        _, loss_val = sess.run([train_op, loss], feed_dict={X:x_data, Y:y_data})\n",
    "        if step%10 ==0:\n",
    "            print(step, loss_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print('Y = {Weight} * X + {Bias}'.format(Weight = sess.run(W), Bias = sess.run(b)))\n",
    "    print('X:5, Y:', sess.run(score, feed_dict={X:5}))\n",
    "    print('X:2.5, Y:', sess.run(score, feed_dict={X:2.5}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
